{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b180c7ff",
   "metadata": {},
   "source": [
    "# Lab 1\n",
    "## Ryan Waterman\n",
    "## 9/2/2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a64b3ec",
   "metadata": {},
   "source": [
    "### Problem 1\n",
    "\n",
    "Follow the steps above to open a Jupyter Notebook and take a screenshot of the output of the code below: \n",
    "\n",
    "```python\n",
    "import torch \n",
    "import torchvision \n",
    "x = torch.rand(5, 3) \n",
    "print(x)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7d3015f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2110, 0.9127, 0.2336],\n",
      "        [0.3110, 0.0235, 0.7488],\n",
      "        [0.3919, 0.2977, 0.3979],\n",
      "        [0.8510, 0.6517, 0.3998],\n",
      "        [0.8340, 0.9969, 0.2435]])\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torchvision \n",
    "x = torch.rand(5, 3) \n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cd49f4",
   "metadata": {},
   "source": [
    "### Problem 2\n",
    "\n",
    "Follow the steps above to open a Jupyter Notebook and take a screenshot of the output of the code below: \n",
    "\n",
    "```python\n",
    "import torch \n",
    "torch.cuda.is_available()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "046e27a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800aae19",
   "metadata": {},
   "source": [
    "### Problem 3:\n",
    "\n",
    "What is the main difference between supervised and unsupervised learning? Is the ResNet model in Chapter 2 an example of supervised or unsupervised learning? Why?\n",
    "\n",
    "**Supervised learning utilizes human generated labels as the \"ground truth\" for testing during the training phase. On the contrary, unsupervised learning does not use labels, and instead learns high dimensional relationships in the data. ResNet is an example of a supervised model, as it was trained on 1.28 million training images, containing a total of 1000 human annotated classes.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9674b881",
   "metadata": {},
   "source": [
    "### Problem 4:\n",
    "\n",
    "Briefly describe the fields of machine learning and deep learning, as well as the main difference(s) between both fields.\n",
    "\n",
    "**Machine Learning:**\n",
    "- **Utilizes algorithmic approaches like decision trees, logistic regression, and KNN to perform classification and prediction tasks on structured data.**\n",
    "- **Requires data preparation and human intervention to ensure optimal results.**\n",
    "\n",
    "**Deep Learning:**\n",
    "- **Utilizes deep, complex neural networks to ingest unstructured data and learn high dimensional relationships.**\n",
    "- **Requires less data preparation and human intervention as patterns are implicitly determined by the model.**\n",
    "- **Far more computationally heavy than ML, as deep learning requires large datasets and model complexity far exceeds ML.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a34e83",
   "metadata": {},
   "source": [
    "### Problem 5:\n",
    "\n",
    "What type of data transformation is performed by a hidden layer in a neural network using RELU activation (f(x)=max(0,x))? What type of data transformation is performed by a hidden layer in a neural network using linear activation (f(x)=x)?\n",
    "\n",
    "**The RELU activation layer clamps the input values between zero and one. The intent of this operation is to introduce non-linearity, as well as resolve the vanishing gradient problem, where gradients in the backpropagation step of training shrink too quickly. This can slow learning, or stop it altogether. The RELU functions derivative is either 0 or 1, \"turning off\" neurons with a negative output, and setting a constant gradient of 1 for all positive outputs.**\n",
    "\n",
    "**Linear activation outputs a value that is the same as the input. This is helpful when utilized as the output layer in regression tasks, where the expected model output is a numerical value. In a hidden layer, this activiation function does not introduce non-linearity, making it an inappropriate selection to assist in learning via gradient descent.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:DSE6211]",
   "language": "python",
   "name": "conda-env-DSE6211-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
